{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Model Training Notebook\n",
    "\n",
    "Train and evaluate models for network anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.data import DataLoader, Preprocessor\n",
    "from src.models import BaselineModels, AnomalyDetector\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "loader = DataLoader('../config.yaml')\n",
    "preprocessor = Preprocessor('../config.yaml')\n",
    "\n",
    "# Try to load data\n",
    "try:\n",
    "    df = loader.load_cicids2017(sample_ratio=0.1)  # Use 10% for quick training\n",
    "except Exception as e:\n",
    "    print(f'Error loading data: {e}')\n",
    "    print('Using sample data...')\n",
    "    df = pd.read_csv('../data/raw/sample_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "df = preprocessor.clean_data(df)\n",
    "df = preprocessor.encode_labels(df)\n",
    "\n",
    "# Prepare features\n",
    "X, y = preprocessor.prepare_features(df)\n",
    "X = preprocessor.scale_features(X)\n",
    "\n",
    "print(f'X shape: {X.shape}')\n",
    "print(f'y shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.split_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE\n",
    "X_train_balanced, y_train_balanced = preprocessor.handle_imbalance(X_train, y_train, method='smote')\n",
    "\n",
    "print(f'Before SMOTE: {len(X_train)}')\n",
    "print(f'After SMOTE: {len(X_train_balanced)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = BaselineModels('../config.yaml')\n",
    "\n",
    "# Train XGBoost\n",
    "models.train('xgboost', X_train_balanced, y_train_balanced, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "class_names = list(preprocessor.label_encoder.classes_)\n",
    "results = models.evaluate('xgboost', X_test, y_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = results['confusion_matrix']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - XGBoost')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importance = models.get_feature_importance('xgboost')\n",
    "feature_names = preprocessor.feature_columns\n",
    "\n",
    "# Sort by importance\n",
    "indices = np.argsort(importance)[-20:]  # Top 20\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(indices)), importance[indices], color='steelblue')\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Random Forest (Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "models.train('random_forest', X_train_balanced, y_train_balanced)\n",
    "rf_results = models.evaluate('random_forest', X_test, y_test, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'Random Forest'],\n",
    "    'Accuracy': [results['accuracy'], rf_results['accuracy']],\n",
    "    'F1 Score': [results['f1_score'], rf_results['f1_score']]\n",
    "})\n",
    "\n",
    "print('Model Comparison:')\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "x = np.arange(len(comparison))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison['Accuracy'], width, label='Accuracy', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, comparison['F1 Score'], width, label='F1 Score', color='coral')\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison['Model'])\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "best_model = 'xgboost' if results['f1_score'] > rf_results['f1_score'] else 'random_forest'\n",
    "print(f'Best model: {best_model}')\n",
    "\n",
    "models.save_model(best_model, '../models')\n",
    "preprocessor.save_artifacts('../models')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
